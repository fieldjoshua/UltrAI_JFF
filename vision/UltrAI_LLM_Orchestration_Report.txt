# UltrAI LLM Orchestration Research & Configuration Report

## Overview
This document consolidates the architecture, configuration logic, and LLM cocktail definitions for the UltrAI orchestration layer. It includes API structure references, concurrency handling notes, and JSON-based fallback logic for OpenRouter-compatible models.

---

## Objective
Enable multiple LLMs (OpenAI, Anthropic, Gemini, GronK, DeepSeek, LLaMA, etc.) to be orchestrated concurrently using OpenRouter’s API. The orchestrator dynamically selects “ready” LLMs from a broad universe and groups them into four core cocktails — **Premium**, **Speedy**, **Budget**, and **Depth** — with pre-defined fallback logic.

---

## Concurrency and Readiness Architecture

### Core Design
1. **Readiness Check Layer**
   - Pings all registered models via OpenRouter API (`/v1/models` endpoint).
   - Filters models marked as `ready: true`.
   - Stores metadata (latency, token limits, provider, cost) for adaptive selection.

2. **Concurrent Execution Layer**
   - Distributes a single user query across all “active” models concurrently using asyncio or multiprocessing pools.
   - Captures and timestamps all responses in structured JSON.

3. **Fallback Mechanism**
   - If a model fails or times out, the orchestrator automatically substitutes it with one of three preassigned backups (defined in the cocktail config).

4. **Dynamic Cocktails**
   - User or system algorithm selects one of four cocktails.
   - Fallback LLMs populate dynamically when primaries are unavailable.

---

## LLM Cocktail Configuration

### JSON Mapping
```json
{
  "version": "1.0",
  "cocktails": {
    "PREMIUM": {
      "openai/gpt-4o": [
        "xai/grok-4",
        "meta/llama-3-70b-instruct",
        "deepseek/deepseek-r1"
      ],
      "anthropic/claude-3.5-sonnet": [
        "xai/grok-4",
        "openai/gpt-4o",
        "google/gemini-2.5-flash"
      ],
      "google/gemini-2.5-pro": [
        "openai/gpt-4o",
        "xai/grok-4",
        "meta/llama-3-70b-instruct"
      ],
      "mistralai/mistral-large": [
        "meta/llama-3-70b-instruct",
        "deepseek/deepseek-r1",
        "xai/grok-4"
      ]
    },
    "SPEEDY": {
      "openai/gpt-4o-mini": [
        "xai/grok-4-fast",
        "anthropic/claude-3.5-sonnet",
        "meta/llama-3-70b-instruct"
      ],
      "anthropic/claude-3-haiku": [
        "openai/gpt-4o-mini",
        "xai/grok-4-fast",
        "google/gemini-2.5-flash-lite"
      ],
      "mistralai/mistral-small": [
        "meta/llama-3-70b-instruct",
        "xai/grok-4-fast",
        "deepseek/deepseek-r1"
      ],
      "deepseek/deepseek-chat": [
        "xai/grok-4-fast",
        "anthropic/claude-3.5-sonnet",
        "openai/gpt-4o-mini"
      ]
    },
    "BUDGET": {
      "openai/gpt-3.5-turbo": [
        "mistralai/mistral-large",
        "meta/llama-2-70b-chat",
        "xai/grok-4-fast"
      ],
      "mistralai/mixtral-8x7b": [
        "mistralai/mistral-large",
        "deepseek/deepseek-r1",
        "anthropic/claude-3-haiku"
      ],
      "meta/llama-3-70b-instruct": [
        "meta/llama-2-70b-chat",
        "mistralai/mistral-large",
        "deepseek/deepseek-r1"
      ],
      "deepseek/deepseek-coder": [
        "deepseek/deepseek-r1",
        "xai/grok-4-fast",
        "meta/llama-2-70b-chat"
      ]
    },
    "DEPTH": {
      "anthropic/claude-3.5-sonnet": [
        "openai/gpt-4o",
        "xai/grok-4",
        "mistralai/mistral-large"
      ],
      "openai/gpt-4o": [
        "anthropic/claude-3.5-sonnet",
        "xai/grok-4",
        "meta/llama-3-70b-instruct"
      ],
      "google/gemini-2.5-pro": [
        "openai/gpt-4o",
        "xai/grok-4",
        "deepseek/deepseek-r1"
      ],
      "deepseek/deepseek-r1": [
        "meta/llama-3-70b-instruct",
        "mistralai/mistral-large",
        "xai/grok-4"
      ]
    }
  }
}
```

---

## Integration Notes

- OpenRouter API endpoint: `https://openrouter.ai/api/v1/chat/completions`
- Authorization: Bearer token (rotated per provider)
- Each concurrent call can use Python’s `asyncio.gather()` to fan out requests
- Suggested HTTP client: `httpx.AsyncClient()` with timeout retries
- Retry strategy: exponential backoff (max 3 attempts per model)
- Health check endpoint: `GET /v1/models`

---

## Reference Integration Snippet (Python)
```python
import asyncio, httpx, os

API_KEY = os.getenv("OPENROUTER_API_KEY")
API_URL = "https://openrouter.ai/api/v1/chat/completions"

async def query_model(model, prompt):
    headers = {"Authorization": f"Bearer {API_KEY}"}
    data = {"model": model, "messages": [{"role": "user", "content": prompt}]}
    async with httpx.AsyncClient(timeout=30) as client:
        resp = await client.post(API_URL, headers=headers, json=data)
        resp.raise_for_status()
        return model, resp.json()["choices"][0]["message"]["content"]

async def orchestrate(prompt, models):
    results = await asyncio.gather(*[query_model(m, prompt) for m in models], return_exceptions=True)
    return {m: r for m, r in results if not isinstance(r, Exception)}
```

---

## Citations
Derived from UltrAI Research Document on Google Drive and accompanying architecture notes 【61†slurm_google_drive†L248-L312】.

---

© 2025 UltrAI Project / Joshua Field. All rights reserved.
